---
repositories:
  registry_community: harbor.cloudical.net
  registry_commercial: harbor.cloudical.net
  package_repository:  https://repo-new.vanillastack.cloudical.net
ingress:
  namespace: nginx-ingress
  enabled: true
  config: {}
  replicas: 3
  chartVersion: "4.0.18"
  defaultBackend:
    replicas: 1
kubernetes:
  dashboard:
    coreDomain: "k8sboard.{{ clusterTLDomain }}"
    chartVersion: "5.3.1"
  clusterName: kubernetes
  version: "1.23.4"
  crioVersion: "1.23"
  crioRuncVersion: "1.0.1~0"
  helm_version: "v3.5.3"
  init_opts: ""
  kubeadm_opts: ""
  drainTimeout: "10m"
  ignoreDrainErrors: true
  cilium:
    version: v1.11.2
  ignorePreflightErrors: []
postgresOperator:
  chartName: "postgres-operator"
  chartVersion: "1.7.1"
  namespace: "postgres-operator"
  backup:
    enabled: false
    s3:
      accessKey: nul
      secretKey: nul
      regionName: nul
      endpoint: nul
      bucketName: nul
    cron: "30 00 * * *"
harbor:
  enabled: true
  chartVersion: 1.8.1
  namespace: harbor
  ingress:
    coreDomain: "harbor.{{ clusterTLDomain }}"
    notaryDomain: "notary.{{ clusterTLDomain }}"
  tls:
    enabled: true
  portal:
    replicas: 1
  core:
    replicas: 1
  jobservice:
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 15Gi
  registry:
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 10Gi
  chartmuseum:
    enabled: true
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 5Gi
  clair:
    enabled: true
    replicas: 1
  trivy:
    enabled: true
    replicas: 1
    persistence:
      storageClass: rook-ceph-block
      size: 5Gi
  notary:
    enabled: true
    replicas: 1
  auth:
    password: 02ZVPokFHXPHfSfkfQGCWIfmJ
    # secret lenght must be 16
    secret: p1zX2AazSCwtUfmv
  redis:
    chartVersion: 10.8.1
    password: zP7GPoCnFNa6XiVnsLVJ3jJgw
    persistence:
      storageClass: "rook-ceph-block"
      size: 8Gi
  postgres:
    size: "20Gi"
    replicas: 2
    version: "12"
    maxConnections: 400
    resources:
      requests:
        cpu: "10m" #default 10m
        memory: "100Mi" #default 100Mi
      limits:
        cpu: "500m" #default 500m
        memory: "500Mi" #default 500Mi
    backup:
      enabled: false
      cronjob: "30 */2 * * *"
efkstack:
  kibana:
    imageTag: "7.10.2"
    user: kibana_test
    password: secretlysecretpassword
    replicas: 2
    system_user: "{{ ansible_user }}"
    system_group: "{{ ansible_user }}"
    coreDomain: "kibana.{{ clusterTLDomain }}"
    resources:
      requests:
        cpu: "200m"
        memory: "1Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
  es:
    imageTag: "7.10.2"
    replicas: 3
    storage:
      class_name: "rook-ceph-block"
      size: "30Gi"
    resources:
      requests:
        cpu: "200m"
        memory: "2Gi"
      limits:
        cpu: "1000m"
        memory: "2Gi"
  logaggregator:
    # Choice of filebeat and fluentd
    name: filebeat
    imageTag: "7.9.1"
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "200Mi"
monitoring:
  admin_user: admin
  admin_password: prompassword
  replicas: 2
  chartVersion: 33.2.1
  retention: 360h
  ingress:
    enabled: true
    dns: "prometheus.{{ clusterTLDomain }}"
  storage:
    classname: "rook-ceph-block"
    size: 30Gi
  resources:
    limits:
      cpu: 2500m
      memory: 2048Mi
    requests:
      cpu: 200m
      memory: 512Mi
  kubeApiServer:
    enabled: true
  kubelet:
    enabled: true
  kubeControllerManager:
    enabled: true
  coreDns:
    enabled: true
  kubeDns:
    enabled: false
  kubeEtcd:
    enabled: true
  kubeScheduler:
    enabled: true
  kubeProxy:
    enabled: true
  kubeStateMetrics:
    enabled: true
  additionalRules:
    cloudical:
      groups:
      - name: Memory_CPU_Using
        rules:
        # reference URL see: https://medium.com/faun/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d
        # - alert: ContainerCpuHighUtilization
        #   expr: ((sum(rate(container_cpu_usage_seconds_total{name!=""}[5m])) BY (pod, namespace)) / (sum(kube_pod_container_resource_limits_cpu_cores) BY (pod, namespace))) * 100 > 80
        #   for: 15m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: 'High CPU Usage Warning for Pod {{"{{ $labels.pod }}"}} on {{"{{ $labels.namespace }}"}}'
        #     description: 'Container CPU usage is above 80%\n  VALUE = {{"{{ $value }}"}}\n  LABELS = {{"{{ $labels.pod }}"}}'
        - alert: ContainerMemoryUsage
          expr: (sum by(container, pod, namespace) (container_memory_working_set_bytes{namespace!~"cert-manager|grafana~|harbor|kube.*|local-path-storage|monitoring|nginx-ingress|postgres-operator|redis|rook-ceph|vanillastack-system|velero"}) / sum by(container, pod, namespace) (kube_pod_container_resource_limits_memory_bytes{}) * 100) > 85
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: 'High Memory Usage Warning for {{"{{ $labels.pod }}"}}'
            description: 'pod {{"{{ $labels.pod }}"}} is using high memory (over 85%)'
        - alert: ContainerMemoryUsage95
          expr: (sum by(container, pod, namespace) (container_memory_working_set_bytes{namespace!=""}) / sum by(container, pod, namespace) (kube_pod_container_resource_limits_memory_bytes{}) * 100) > 95
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: 'High Memory Usage Warning for {{"{{ $labels.pod }}"}}'
            description: 'pod {{"{{ $labels.pod }}"}} is using high memory (over 95%)'
        - alert: ContainerOOMKiller
          expr: (kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}) > 0 and rate(kube_pod_container_status_restarts_total{namespace=~".*"}[5m]) * 60 * 5 > 1
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: 'Pod-killed (OOM) CRITICAL for {{"{{ $labels.pod }}"}}'
            description: 'pod {{"{{ $labels.pod }}"}} was killed because OOM'
        - alert: PodTerminating
          expr: (sum by (container, pod, namespace) (kube_pod_container_status_terminated_reason{reason="Terminating"} offset 1m )) >= 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: 'Pod-terminted WARNING for {{"{{ $labels.pod }}"}}'
            description: 'pod {{"{{ $labels.pod }}"}} is tarminated longer for 1 minute'
alertmanager:
  enabled: true
  admin_user: admin
  admin_password: ampassword
  replicas: 2
  ingress:
    enabled: true
    dns: "alertmanager.{{ clusterTLDomain }}"
  storage:
    classname: "rook-ceph-block"
    size: 10Gi
  resources:
    limits:
      cpu: 2500m
      memory: 2048Mi
    requests:
      cpu: 200m
      memory: 512Mi
  config: |
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
    receivers:
    - name: 'null'
grafana:
  namespace: grafana
  chartVersion: 6.24.1 
  replicas: 1
  enabled: true
  admin_user: admin
  admin_password: grafanapassword
  ingress:
    enabled: true
    dns: "grafana.{{ clusterTLDomain }}"
  persistence:
    type: pvc
    enabled: false
    storageClassName: rook-ceph-block
    accessModes:
      - ReadWriteOnce
    size: 15Gi
    # annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
loki:
  namespace: loki
  chartVersion: 2.3.1
  replicas: 1
  monitoring:
    enabled: true
  persistence:
    enabled: true
    size: 30Gi
    storageClass: rook-ceph-block
keycloak:
  namespace: keycloak
  externalDatabase: false
  postgres:
    size: "20Gi"
    replicas: 2
    version: "12"
    maxConnections: 400
    backup:
      enabled: false
      cronjob: "30 */2 * * *"
  ingress:
    enabled: true
    publicDomain: "keycloak.{{ clusterTLDomain }}"
  realm:
    name: "{{ clusterTLDomain }}"
    id: "{{ clusterTLDomain }}"
