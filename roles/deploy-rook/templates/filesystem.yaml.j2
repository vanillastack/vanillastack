#################################################################################################################
# Create a filesystem with settings with replication enabled for a production environment.
# A minimum of 3 OSDs on different nodes are required in this example.
#  kubectl create -f filesystem.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: {{rook.filesystem.name}}
  namespace: {{rook.namespace}}
spec:
  # The metadata pool spec. Must use replication.
  metadataPool:
    replicated:
      size: {{rook.filesystem.replicaLevel}}
    parameters:
      min_size: "{{rook.filesystem.minSize}}"
      # Inline compression mode for the data pool
      # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression
      compression_mode: none
        # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
      # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
      #target_size_ratio: .5
  # The list of data pool specs. Can use replication or erasure coding.
  dataPools:
    - failureDomain: {{rook.filesystem.failureDomain}}
      replicated:
        size: {{rook.filesystem.replicaLevel}}
        # Disallow setting pool with replica 1, this could lead to data loss without recovery.
        # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
        # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
        # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
        #targetSizeRatio: .5
      parameters:
        min_size: "{{rook.filesystem.minSize}}"
        # Inline compression mode for the data pool
        # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression
        compression_mode: none
          # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
        # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
        #target_size_ratio: .5
  # Whether to preserve metadata and data pools on filesystem deletion
  preservePoolsOnDelete: true
  # The metadata service (mds) configuration
  metadataServer:
    # The number of active MDS instances
    activeCount: 1
    # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
    # If false, standbys will be available, but will not have a warm cache.
    activeStandby: true
    # The affinity rules to apply to the mds deployment
    placement:
    #  nodeAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      nodeSelectorTerms:
    #      - matchExpressions:
    #        - key: role
    #          operator: In
    #          values:
    #          - mds-node
    #  topologySpreadConstraints:
    #  tolerations:
    #  - key: mds-node
    #    operator: Exists
    #  podAffinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - rook-ceph-mds
          # topologyKey: kubernetes.io/hostname will place MDS across different hosts
          topologyKey: kubernetes.io/hostname
    # A key/value list of annotations
    annotations:
    #  key: value
    resources:
      limits:
        cpu: "750m"
        memory: "4096Mi"
      requests:
        cpu: "150m"
        memory: "3072Mi"
    priorityClassName: system-cluster-critical
